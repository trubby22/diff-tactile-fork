"""
cma.py
Implemented August, 2023
Use implemented class TactileEnv to train cma-se algorithm
A sample based trajectory optimization algorithm
use multiprocessing to accelerate computation
For use of pycma
"""

import cma
import os
import argparse
import taichi as ti
import numpy as np
import matplotlib.pyplot as plt
from register_tasks import ContactTask, get_tasks
##################################################################
# init taichi env & use implemented Contact class
##################################################################

##################################################################
# NOTE: if you only want to use some of available gpu, set it here
devices = ["0"]


# os.environ['CUDA_VISIBLE_DEVICES'] = devices
##################################################################
# implement CMA_Trainer,
# use implemented pycma to train taichi env tasks
##################################################################
class CMA_Trainer:
    def __init__(
        self,
        task: ContactTask,
        total_opt_step,
        sigma0,
        resume,
        off_screen,
        render,
        save_path,
        ask_len,
        use_state,
        use_tactile,
    ):
        self.task = task
        self.n_actions = task.n_actions
        self.sub_step = task.sub_steps
        self.total_step = task.total_steps
        self.total_opt_step = total_opt_step
        self.sigma0 = sigma0
        self.resume = resume
        self.render = render
        self.x0 = task.x0
        self.ask_len = ask_len
        self.off_screen = off_screen
        self.training_steps = []
        self.mean_rewards = []
        self.std_rewards = []
        self.traj_reward = []
        self.save_path = save_path
        self.min_reward = np.inf
        self.use_state = use_state
        self.use_tactile = use_tactile
        self.es = cma.CMAEvolutionStrategy(
            self.x0, self.sigma0, {"popsize": self.ask_len}
        )
        self.contact_model = task.contact_model(
            use_tactile=self.use_tactile,
            use_state=self.use_state,
            dt=task.dt,
            total_steps=task.total_steps,
            sub_steps=task.sub_steps,
            obj=task.obj,
        )

    def reset(self, seed=None, options=None):
        self.contact_model.init()
        self.contact_model.clear_all_grad()
        self.contact_model.reset()

    def evaluate_traj(self, x):
        """_summary_

        Args:
            x (numpy.array): trajetory generated by pycma

        Returns:
            reward: total reward / loss
        """
        if self.render:
            gui1 = ti.GUI("Contact Viz")
            gui2 = ti.GUI("Force Map 1")
            gui3 = ti.GUI("Deformation Map 1")
        reward = 0
        self.reset()
        actions = x.reshape(self.total_step, self.n_actions)
        for ts in range(self.total_step):
            self.step(actions[ts], ts)
            truncate = self.check_truncation()
            reward = self.compute_reward(ts)
            if self.render and self.task.task_name != "cable_manip":
                self.contact_model.render(gui1, gui2, gui3)
            if truncate:
                print("truncate at ", ts)
                break
            if ts == self.total_step - 1:
                print("finish:", ts)
        print("reward:", reward)
        return reward

    def check_truncation(self):
        """_summary_
        Returns:
            Bool: sometimes taichi env can crush, check if marker includes nan
            if nan, return True, else False
        """
        if self.task.task_name == "cable_manip":
            self.contact_model.gripper.fem_sensor1.extract_markers(0)
            return np.isnan(
                self.contact_model.gripper.fem_sensor1.predict_markers.to_numpy()
            ).any()
        else:
            self.contact_model.fem_sensor1.extract_markers(0)
            return np.isnan(
                self.contact_model.fem_sensor1.predict_markers.to_numpy()
            ).any()

    def compute_reward(self, ts):
        """_summary_
            compute reward / loss from taichi env API
        Returns:
            loss: cma optimize to get lower value in tell_list
            loss here should be positive and should be better to be smaller
        """
        self.contact_model.compute_loss(ts)
        return self.contact_model.loss[None]

    def step(self, action, ts):
        self.contact_model.apply_action(action, ts)
        self.contact_model.calculate_force(ts)

    def train(self):
        """_summary_
        train given task with pycma repo
        will update training image each optimization step
        """
        for ww in range(self.total_opt_step):
            X = self.es.ask()
            tell_list = []
            for x in X:
                tell_list.append(self.evaluate_traj(x))
            self.es.tell(X, tell_list)
            self.es.disp()
            highest_reward = np.max(tell_list)
            if highest_reward >= self.best_reward:
                self.best_reward = highest_reward
                np.save(
                    os.path.join(self.save_path, "best_traj.npy"),
                    X[np.argmax(tell_list)],
                )
            mean_reward = np.mean(tell_list)
            std_reward = np.std(tell_list)
            self.mean_rewards.append(mean_reward)
            self.std_rewards.append(std_reward)
            self.training_steps.append(ww)
            self.draw_training_image()

    def draw_training_image(self):
        """_summary_
        used to draw training loss / reward curve
        including mean and stddev
        """
        img_path = self.save_path + "training_curve.png"
        plt.plot(self.training_steps, self.mean_rewards, color="red")
        plt.fill_between(
            self.training_steps,
            np.array(self.mean_rewards) - np.array(self.std_rewards),
            np.array(self.mean_rewards) + np.array(self.std_rewards),
            color="blue",
            alpha=0.2,
        )
        plt.title("Training Curve for cma_" + self.task.task_name)
        plt.xlabel("Timestep")
        plt.ylabel("Reward")
        plt.savefig(img_path)

    def load(self):
        return np.load(os.path.join(self.save_path, "best_traj.npy"))

    def evaluate(self):
        """_summary_
        render guis as taichi envs do
        use best_traj.npy in self.save_path
        """
        eval_rwd = 0
        if not self.off_screen:
            gui1 = ti.GUI("Contact Viz")
            gui2 = ti.GUI("Force Map 1")
            gui3 = ti.GUI("Deformation Map 1")
        traj = self.load().reshape(self.total_step, self.n_actions)
        self.contact_model.reset()
        for ts in range(self.total_step):
            self.step(traj[ts], ts)
            eval_rwd += self.compute_reward(ts)
            if not self.off_screen:
                self.contact_model.render(gui1, gui2, gui3)
        print("total reward:", eval_rwd)


##################################################################
# training with CMA_Trainer
##################################################################
if __name__ == "__main__":
    ti.init(debug=False, offline_cache=False, arch=ti.gpu, device_memory_GB=9)
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--use_state", action="store_true", help="whether to use state loss"
    )
    parser.add_argument(
        "--use_tactile", action="store_true", help="whether to use tactile loss"
    )
    parser.add_argument(
        "--off_screen",
        action="store_true",
        help="whether or not set mode to off screen",
    )
    parser.add_argument(
        "--resume", action="store_true", help="whether or not resume best_model"
    )
    parser.add_argument(
        "--render", action="store_true", help="whether or not render visualiztion"
    )
    parser.add_argument("--task", default="box_open", help="task type")
    parser.add_argument(
        "--opt_steps", type=int, default=5, help="total steps to optimize network"
    )
    parser.add_argument(
        "--ask_len",
        type=int,
        default=20,
        help="number of trajectories each optimization step",
    )
    args = parser.parse_args()

    task = get_tasks(args.task)
    if task == None:
        raise NotImplementedError
    sigma0 = 0.015

    save_path = (
        "checkpoint_"
        + args.task
        + f"/cmaes/state_{args.use_state}_force_{args.use_tactile}/"
    )
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    cma_trainer = CMA_Trainer(
        task=task,
        sigma0=sigma0,
        save_path=save_path,
        total_opt_step=args.opt_steps,
        resume=args.resume,
        off_screen=args.off_screen,
        render=args.render,
        ask_len=args.ask_len,
        use_tactile=args.use_tactile,
        use_state=args.use_state,
    )
    cma_trainer.train()
